{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Tensor objects for storing and updating model parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special tensor object for which gradients need to be computed allows us to store and update the parameters of our models during training. Such a tensor can be created by just assigning \"requires_grad\" to True on user-specified initial values. Note that as of now, only tensors of floating point and complex dtype can require gradients. In the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1400, requires_grad=True)\n",
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor(3.14, requires_grad=True)\n",
    "b = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) \n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that \"requires_grad\" is set to False by default. This value can be efficiently set to True by running \"requires_grad_()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(w.requires_grad)\n",
    "w.requires_grad_()\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients via automatic differentiation and GradientTape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the gradients of the loss with respect to variables\n",
    "PyTorch supports automatic differentiation, which can be thought of as an implementation of the chain rule for computing gradients of nested functions. To compute these gradients, we can call the \"backward\" method from the \"torch.autograd\" module. It computes the sum of gradients of the given tensor with regard to leaf nodes (terminal nodes) in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw :  tensor(-0.5600)\n",
      "dL/db :  tensor(-0.4000)\n",
      "tensor([-0.5600], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.5, requires_grad=True) \n",
    "\n",
    "x = torch.tensor([1.4])\n",
    "y = torch.tensor([2.1])\n",
    "\n",
    "\n",
    "z = torch.add(torch.mul(w, x), b) # forward pass\n",
    " \n",
    "loss = (y-z).pow(2).sum() # the sum is for multiple output and redundant here\n",
    "loss.backward()\n",
    "\n",
    "print('dL/dw : ', w.grad)\n",
    "print('dL/db : ', b.grad)\n",
    "\n",
    "# verifying the computed gradient dL/dw\n",
    "print(2 * x * ((w * x + b) - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding automatic differentiation\n",
    "Automatic differentiation represents a set of computational techniques for computing gradients of arbitrary arithmetic operations. During this process, gradients of a computation (expressed as a series of operations) are obtained by accumulating the gradients through repeated applications of the chain rule. To better understand the concept behind automatic differentiation, let’s consider a series of nested computations, $y = f(g(h(x)))$, with input $x$ and output $y$. This can be broken into a series of steps:\n",
    "\\begin{align*}\n",
    "u_0 &= x \\\\\n",
    "u_1 &= h(x) \\\\\n",
    "u_2 &= g(u_1) \\\\\n",
    "u_3 &= f(u_2) = y\n",
    "\\end{align*}\n",
    "Hence,\n",
    "\\begin{align*}\n",
    "\\frac{dy}{dx} = \\frac{dy}{du_2} \\times \\frac{du_2}{du_1} \\times \\frac{du_1}{dx}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial examples\n",
    "Computing gradients of the loss with respect to the input example is used for generating adversarial examples (or adversarial attacks). In computer vision, adversarial examples are examples that are generated by adding some small, imperceptible noise (or perturbations) to the input example, which results in a deep NN misclassifying them. Covering adversarial examples is beyond the scope of this book, but if you are interested, you can find the original paper by Christian Szegedy et al., Intriguing properties of neural networks at https://arxiv.org/pdf/1312.6199.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying implementations of common architectures via the \"torch.nn\" module\n",
    "We have already seen some examples of building a feedforward NN model (for instance, a multilayer perceptron) and defining a sequence of layers using the \"nn.Module\" class. Before we take a deeper dive into \"nn.Module\", let’s briefly look at another approach for conjuring those layers via \"nn.Sequential\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing models based on \"nn.Sequential\"\n",
    "With nn.Sequential (https://pytorch.org/docs/master/generated/torch.nn.Sequential.html#sequential), the layers stored inside the model are connected in a cascaded way. In the following example, we will build a model with two densely (fully) connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 32),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a loss function"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a92481e70822987d79ebd7dc62d603ec0859c6fb39528011355d2e4493612803"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
