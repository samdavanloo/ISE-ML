{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/samdavanloo/ISE-ML/blob/main/2_regularization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in model fitting\n",
    "Regularization is one approach to tackle the problem of overfitting by adding additional information, and thereby shrinking the parameter values of the\n",
    "model to induce a penalty against complexity. The most popular approaches to regularized linear regression are the so-called \"Ridge Regression\", \"Least Absolute Shrinkage and Selection Operator (LASSO)\", and \"Elastic Net\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "Ridge regression is an L2-norm penalized model where we simply add the squared sum of the weights to our least-squares cost function:\n",
    "$$\n",
    "l_{\\text{Ridge}}(\\mathbf{w}) = \\sum_{i=1}^n(y_i-\\hat{f}_i(\\mathbf{w},w_0))^2 + \\lambda||\\mathbf{w}||_2^2,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "||\\mathbf{w}||_2^2 \\triangleq \\sum_{j=1}^p w_j^2.\n",
    "$$\n",
    "By increasing the value of hyperparameter $\\lambda$, we increase the regularization strength and shrink the weights of our model. Note that we don't regularize the intercept term $w_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Absolute Shrinkage and Selection Operator (LASSO)\n",
    "An alternative approach that can lead to sparse models is LASSO. Depending on the regularization strength, certain weights can become zero, which also makes LASSO useful as a supervised feature selection technique. The optimization problem behind fitting LASSO has the form of\n",
    "$$\n",
    "l_{\\text{LASSO}}(\\mathbf{w}) = \\sum_{i=1}^n(y_i-\\hat{f}_i(\\mathbf{w},w_0))^2 + \\lambda||\\mathbf{w}||_1,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "||\\mathbf{w}||_1 \\triangleq \\sum_{j=1}^p |w_j|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "A limitation of LASSO is that it selects at most n variables if p>n. A compromise between Ridge regression and LASSO is Elastic Net, which has an L1 penalty to generate sparsity and an L2 penalty to overcome some of the limitations of LASSO, such as the number of selected variables:\n",
    "$$\n",
    "l_{\\text{Ridge}}(\\mathbf{w}) = \\sum_{i=1}^n(y_i-\\hat{f}_i(\\mathbf{w},w_0))^2 + \\lambda_1||\\mathbf{w}||_2^2+ \\lambda_2||\\mathbf{w}||_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elanet = ElasticNet(alpha=1.0, l1_ratio=0.5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
