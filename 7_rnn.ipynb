{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "Typical machine learning algorithms for supervised learning assume that the input is independent and identically distributed (IID) data, which means that the training examples are mutually independent and have the same underlying distribution. In this regard, based on the mutual independence assumption, the order in which the training examples are given to the model is irrelevant. For example, if we have a sample consisting of n training examples, x(1), x(2), ..., x(n), the order in which we use the data for training our machine learning algorithm does not matter.\n",
    "\n",
    "What makes sequences unique, compared to other types of data, is that elements in a sequence appear in a certain order and are not independent of each other, i.e., order matters. Predicting the market value of a particular stock would be an example of this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential data vs time series data\n",
    "Time series data is a special type of sequential data where each example is associated with a dimension for time. In time series data, samples are taken at successive timestamps, and therefore, the time dimension determines the order among the data points. For example, stock prices and voice or speech records are time series data.\n",
    "\n",
    "On the other hand, not all sequential data has the time dimension. For example, in text data or DNA sequences, the examples are ordered, but text or DNA does not qualify as time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing seqeunces\n",
    "In the ML context, both features and labels are indexed by time as $(\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)},\\cdots, \\mathbf{x}^{(T)})$ and $(y^{(1)}, y^{(2)},\\cdots, y^{(T)})$. RNNs are designed for modeling sequences and are capable of remembering past information (they have memory) and processing new events accordingly, which is a clear advantage when working with sequence data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different categrories of sequence data\n",
    "If either the input or output is a sequence, the modelin task falls into one of these categors:\n",
    "\n",
    "(Many-to-one)  Input: sequence, Output: not sequence. Exampel: Classifying whether a person liked a movie based on her comment.\n",
    "\n",
    "(One-to-many) Input: not sequcne, output: sequence, Example: Image (not sequence) captioning (seqeunce)\n",
    "\n",
    "(Many-to-many) Both sequence. Example: translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an RNN, the hidden layer receives its input from both the input layer of the current time step and the hidden layer from the previous time step. Since, in this case, each recurrent layer must receive a sequence as input, all the recurrent layers except the last one must return a sequence as output (that is, we will later have to set \"return_sequences=True\"). The behavior of the last recurrent layer depends on the type of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing activations in an RNN\n",
    "We should note that weight matrices in RNN do not depend on time $t$, i.e., they are shared across time axis. Computing the activation is very much similar to standard multilayers perceptron:\n",
    "\\begin{align}\n",
    "\\mathbf{z}^{(t)}_h &= W_{xh}\\mathbf{x}^{(t)} + W_{hh}\\mathbf{h}^{(t-1)} + \\mathbf{b}_h \\\\\n",
    "\\mathbf{h}^{(t)} &= \\sigma_h(\\mathbf{z}^{(t)}_h) \n",
    "\\end{align}\n",
    "The activation of th eoutput unit can be computed as\n",
    "\\begin{align}\n",
    "\\mathbf{o}^{(t)} = \\sigma_0(W_{ho}h^{(t)}) + \\mathbf{b}_o \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea to derive the gradient of the composition function is similar to before, while a bit more complicated. The overall loss, $L$, is the sum of all loss function at $t=1,\\cdots,T$:\n",
    "\\begin{align}\n",
    "L = \\sum_{t=1}^T L^{(t)}\n",
    "\\end{align}\n",
    "Since the loss at time $t$ depends on the hidden units at all previous times $1:t$, the gradient will be calculated as\n",
    "\\begin{align}\n",
    "\\frac{\\partial L^{(t)}}{W_{hh}} = \\frac{\\partial L^{(t)}}{o^{(t)}} \\times \\frac{\\partial o^{(t)}}{h^{(t)}} \\times (\\sum_{k=1}^t \\frac{\\partial h^{(t)}}{h^{(k)}} \\times \\frac{\\partial h^{(k)}}{W_{hh}})\n",
    "\\end{align}\n",
    "where \n",
    "\\begin{align}\n",
    "\\frac{\\partial h^{(t)}}{h^{(k)}}  = \\Pi_{i=k+1}^t \\frac{\\partial h^{(i)}}{h^{(i-1)}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output-to-hidden and output-to-output recurrence\n",
    "So far, we saw hidden-to-hidden recurrence. However, we potentially can have output-to-hidden and output-to-output recurrence too (check the corresponding slide). To see how this works in practice, let’s manually compute the forward pass for one of these recurrent types. recurrence. In the following code, we will create a recurrent layer from RNN and perform a forward pass on an input sequence of length 3 to compute the output. We will also manually compute the forward pass and compare the results with those of RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: torch.Size([2, 5])\n",
      "W_hh shape: torch.Size([2, 2])\n",
      "b_xh shape: torch.Size([2])\n",
      "b_hh shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=1, batch_first=True) \n",
    "\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_hh shape:', w_hh.shape)\n",
    "print('b_xh shape:', b_xh.shape)\n",
    "print('b_hh shape:', b_hh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden          : [[-0.4701929  0.5863904]]\n",
      "   Output (manual) : [[-0.3519801   0.52525216]]\n",
      "   RNN output      : [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden          : [[-0.88883156  1.2364397 ]]\n",
      "   Output (manual) : [[-0.68424344  0.76074266]]\n",
      "   RNN output      : [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden          : [[-1.3074701  1.886489 ]]\n",
      "   Output (manual) : [[-0.8649416   0.90466356]]\n",
      "   RNN output      : [[-0.8649416   0.90466356]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "\n",
    "## output of the simple RNN:\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh    \n",
    "    print('   Hidden          :', ht.detach().numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_h = out_man[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.detach().numpy())\n",
    "    print('   RNN output      :', output[:, t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenges of learning long-range interactions\n",
    "Back propagation through time introduces some new challenges. Because of the multiplicative factor, in computing the gradients of a loss function, the so-called \"vanishing and exploding gradient\" problems arise. Basically, $\\partial h^{(t)}/\\partial h^{(k)}$ has $t-k$ multiplications; therefore, multiplying the weight, $W$, by itself $t – k$ times. As a result, if $|W| < 1$, this factor becomes very small when $t – k$ is large. On the other hand, if the weight of the recurrent edge is $|W| > 1$, then $W^{t–k}$ becomes very large when t – k is large. Note that a large t – k refers to long-range dependencies.\n",
    "\n",
    "Solutions: 1. Gradient clipping, 2. Truncated backpropagation through time, 3. \"LSTM\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a92481e70822987d79ebd7dc62d603ec0859c6fb39528011355d2e4493612803"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
